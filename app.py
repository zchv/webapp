"""
CLIP è¯­ä¹‰å›¾åƒæœç´¢ - é¢„è®¡ç®—Embeddingsç‰ˆæœ¬
ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„å›¾ç‰‡å‘é‡è¿›è¡Œå¿«é€Ÿæœç´¢
"""

import streamlit as st
import torch
import open_clip
from PIL import Image
import pickle
import os
import numpy as np

# Page configuration
st.set_page_config(
    page_title="CLIP è¯­ä¹‰å›¾åƒæœç´¢",
    page_icon="ğŸ”",
    layout="wide"
)

# é…ç½®
EMBEDDINGS_FILE = "image_embeddings.pkl"
MODEL_NAME = 'ViT-B-32'
PRETRAINED = './models/ViT-B-32-laion2B-s34B-b79K/open_clip_pytorch_model.bin'

@st.cache_resource
def load_clip_model():
    """åŠ è½½CLIPæ¨¡å‹"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, _, preprocess = open_clip.create_model_and_transforms(
        MODEL_NAME,
        pretrained=PRETRAINED
    )
    model = model.to(device)
    model.eval()
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    return model, tokenizer, device

@st.cache_data
def load_embeddings():
    """åŠ è½½é¢„è®¡ç®—çš„å›¾ç‰‡embeddings"""
    if not os.path.exists(EMBEDDINGS_FILE):
        return None

    with open(EMBEDDINGS_FILE, 'rb') as f:
        data = pickle.load(f)

    return data

def enhance_query(query):
    """å¢å¼ºæŸ¥è¯¢æ–‡æœ¬"""
    query = query.strip()

    # å¦‚æœæŸ¥è¯¢å¾ˆçŸ­ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
    if len(query.split()) <= 2:
        if not query.lower().startswith(('a ', 'an ', 'the ', 'ä¸€ä¸ª', 'ä¸€å¼ ')):
            query = f"a photo of {query}"

    return query

def search_images(query, embeddings_data, model, tokenizer, device, top_k=20,
                 use_query_enhancement=True, temperature=1.0):
    """ä½¿ç”¨è‡ªç„¶è¯­è¨€æœç´¢å›¾ç‰‡"""
    # æŸ¥è¯¢å¢å¼º
    if use_query_enhancement:
        enhanced_query = enhance_query(query)
    else:
        enhanced_query = query

    # ç¼–ç æ–‡æœ¬
    text = tokenizer([enhanced_query]).to(device)

    with torch.no_grad():
        # æå–æ–‡æœ¬ç‰¹å¾
        text_features = model.encode_text(text)
        # L2å½’ä¸€åŒ–
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

    # è½¬æ¢ä¸ºnumpy
    text_features_np = text_features.cpu().numpy()

    # è®¡ç®—ç›¸ä¼¼åº¦
    embeddings = embeddings_data['embeddings']
    similarities = (embeddings @ text_features_np.T).squeeze()

    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    if temperature != 1.0:
        similarities = similarities / temperature

    # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    similarities_percent = similarities * 100.0

    # è·å–top kç»“æœ
    top_indices = np.argsort(similarities_percent)[::-1][:top_k]

    results = []
    for idx in top_indices:
        results.append({
            'path': embeddings_data['image_paths'][idx],
            'score': float(similarities_percent[idx])
        })

    return results, enhanced_query

# Main app
st.title("ğŸ” CLIP è¯­ä¹‰å›¾åƒæœç´¢")
st.markdown("ä½¿ç”¨è‡ªç„¶è¯­è¨€æœç´¢ä½ çš„å›¾ç‰‡åº“ï¼")

# åŠ è½½æ¨¡å‹
with st.spinner("åŠ è½½CLIPæ¨¡å‹..."):
    model, tokenizer, device = load_clip_model()
st.success(f"âœ… æ¨¡å‹å·²åŠ è½½ ({device})")

# åŠ è½½embeddings
embeddings_data = load_embeddings()

if embeddings_data is None:
    st.error("âŒ æœªæ‰¾åˆ°å›¾ç‰‡embeddingsæ–‡ä»¶ï¼")
    st.markdown("""
    ### ğŸ“ ä½¿ç”¨æ­¥éª¤

    1. **æ·»åŠ å›¾ç‰‡** åˆ° `./images` æ–‡ä»¶å¤¹
    2. **ç”Ÿæˆembeddings** è¿è¡Œ:
       ```bash
       python get_embeddings.py
       ```
    3. **åˆ·æ–°** æ­¤é¡µé¢

    ğŸ’¡ Embeddingsåªéœ€ç”Ÿæˆä¸€æ¬¡ï¼Œæˆ–åœ¨æ·»åŠ æ–°å›¾ç‰‡æ—¶é‡æ–°ç”Ÿæˆã€‚
    """)
    st.stop()

# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
num_images = len(embeddings_data['image_paths'])
st.info(f"ğŸ“Š å·²åŠ è½½ {num_images} å¼ å›¾ç‰‡çš„embeddings")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ æœç´¢è®¾ç½®")

    use_enhancement = st.checkbox("æŸ¥è¯¢å¢å¼º", value=True,
                                  help="è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢æ–‡æœ¬ä»¥æé«˜æ£€ç´¢æ•ˆæœ")

    temperature = st.slider("æ¸©åº¦", min_value=0.1, max_value=2.0, value=1.0, step=0.1,
                           help="è°ƒæ•´ç›¸ä¼¼åº¦åˆ†å¸ƒã€‚<1: æ›´é›†ä¸­ï¼Œ>1: æ›´åˆ†æ•£")

    st.markdown("---")
    st.markdown("### â„¹ï¸ ä¿¡æ¯")
    st.markdown(f"""
    **æ¨¡å‹**: {MODEL_NAME} (LAION)
    **è®¾å¤‡**: {device}
    **å›¾ç‰‡æ•°é‡**: {num_images}
    **Embeddingç»´åº¦**: 512
    """)

    st.markdown("---")
    st.markdown("### ğŸ”„ æ›´æ–°å›¾ç‰‡")
    st.markdown("""
    æ·»åŠ æ–°å›¾ç‰‡å:
    1. å°†å›¾ç‰‡æ”¾å…¥ `./images` æ–‡ä»¶å¤¹
    2. è¿è¡Œ: `python get_embeddings.py`
    3. åˆ·æ–°æ­¤é¡µé¢
    """)

# ä¸»å†…å®¹åŒºåŸŸ
st.markdown("---")

# æœç´¢ç•Œé¢
col1, col2 = st.columns([4, 1])
with col1:
    query = st.text_input(
        "ğŸ” è¾“å…¥æœç´¢æŸ¥è¯¢",
        placeholder="ä¾‹å¦‚: ä¸€ä¸ªå¾®ç¬‘çš„äºº, red car, sunset, ç¾é£Ÿ..."
    )
with col2:
    top_k = st.number_input("æ˜¾ç¤ºç»“æœæ•°", min_value=5, max_value=100, value=20, step=5)

# æœç´¢æç¤º
with st.expander("ğŸ’¡ æœç´¢æŠ€å·§"):
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**âœ… å¥½çš„æŸ¥è¯¢**")
        st.markdown("""
        - "a golden retriever dog running in park"
        - "modern glass building with blue sky"
        - "delicious pizza on wooden table"
        - "ä¸€ä¸ªç©¿çº¢è¡£æœçš„å¥³å­©åœ¨æµ·è¾¹"
        """)

    with col2:
        st.markdown("**âŒ é¿å…çš„æŸ¥è¯¢**")
        st.markdown("""
        - "dog" (å¤ªç®€å•)
        - "thing" (å¤ªæ¨¡ç³Š)
        - "nice" (å¤ªä¸»è§‚)
        - "it" (æ— æ„ä¹‰)
        """)

    st.markdown("---")
    st.markdown("**ğŸ¯ æé«˜æ•ˆæœçš„æ–¹æ³•:**")
    st.markdown("""
    1. ä½¿ç”¨æè¿°æ€§è¯­è¨€ï¼ŒåŒ…å«é¢œè‰²ã€åŠ¨ä½œã€åœºæ™¯ç­‰ç»†èŠ‚
    2. åŒ…å«å…³é”®ç‰¹å¾ï¼Œæè¿°ä¸»è¦è§†è§‰å…ƒç´ 
    3. ä½¿ç”¨å…·ä½“è¯æ±‡ï¼Œé¿å…æŠ½è±¡æˆ–æ¨¡ç³Šçš„è¯
    4. ç»„åˆå¤šä¸ªæ¦‚å¿µï¼Œæè¿°å®Œæ•´åœºæ™¯
    5. æ”¯æŒä¸­è‹±æ–‡ï¼Œä¸¤ç§è¯­è¨€éƒ½èƒ½è·å¾—å¥½æ•ˆæœ
    """)

if query:
    with st.spinner("æœç´¢ä¸­..."):
        results, enhanced_query = search_images(
            query,
            embeddings_data,
            model,
            tokenizer,
            device,
            int(top_k),
            use_query_enhancement=use_enhancement,
            temperature=temperature
        )

    # æ˜¾ç¤ºå¢å¼ºåçš„æŸ¥è¯¢
    if use_enhancement and enhanced_query != query:
        st.info(f"ğŸ”„ å¢å¼ºæŸ¥è¯¢: `{enhanced_query}`")

    st.markdown(f"### æœç´¢ç»“æœ: *'{query}'*")

    # æ˜¾ç¤ºç›¸ä¼¼åº¦ç»Ÿè®¡
    scores = [r['score'] for r in results]
    if scores:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æœ€é«˜åˆ†", f"{max(scores):.1f}%")
        with col2:
            st.metric("å¹³å‡åˆ†", f"{np.mean(scores):.1f}%")
        with col3:
            st.metric("æœ€ä½åˆ†", f"{min(scores):.1f}%")

    # æ˜¾ç¤ºç»“æœç½‘æ ¼
    cols_per_row = 4
    for i in range(0, len(results), cols_per_row):
        cols = st.columns(cols_per_row)
        for j, col in enumerate(cols):
            if i + j < len(results):
                result = results[i + j]
                with col:
                    try:
                        img = Image.open(result['path'])
                        st.image(img, use_container_width=True)

                        # é¢œè‰²ç¼–ç ç›¸ä¼¼åº¦
                        score = result['score']
                        if score >= 30:
                            color = "ğŸŸ¢"
                        elif score >= 20:
                            color = "ğŸŸ¡"
                        else:
                            color = "ğŸ”´"

                        st.caption(f"{color} **{score:.1f}%**")
                        st.caption(f"{os.path.basename(result['path'])}")
                    except Exception as e:
                        st.error(f"åŠ è½½å›¾ç‰‡å‡ºé”™: {e}")

else:
    # æ˜¾ç¤ºç¤ºä¾‹æŸ¥è¯¢
    st.markdown("### ğŸ’¡ ç¤ºä¾‹æŸ¥è¯¢")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**è‹±æ–‡æŸ¥è¯¢:**")
        st.markdown("""
        - "a person smiling happily"
        - "red sports car on street"
        - "delicious food on white plate"
        - "beautiful sunset over mountains"
        - "modern office interior"
        - "cute dog playing in park"
        """)

    with col2:
        st.markdown("**ä¸­æ–‡æŸ¥è¯¢:**")
        st.markdown("""
        - "ä¸€ä¸ªå¼€å¿ƒå¾®ç¬‘çš„äºº"
        - "è¡—é“ä¸Šçš„çº¢è‰²è·‘è½¦"
        - "ç™½è‰²ç›˜å­ä¸Šçš„ç¾é£Ÿ"
        - "å±±ä¸Šçš„ç¾ä¸½æ—¥è½"
        - "ç°ä»£åŠå…¬å®¤å†…æ™¯"
        - "å…¬å›­é‡Œç©è€çš„å¯çˆ±å°ç‹—"
        """)

    # æ˜¾ç¤ºéƒ¨åˆ†æ ·ä¾‹å›¾ç‰‡
    with st.expander("ğŸ“¸ æŸ¥çœ‹éƒ¨åˆ†å›¾ç‰‡æ ·ä¾‹", expanded=False):
        sample_size = min(12, num_images)
        sample_indices = np.random.choice(num_images, sample_size, replace=False)

        cols = st.columns(4)
        for idx, img_idx in enumerate(sample_indices):
            with cols[idx % 4]:
                try:
                    img_path = embeddings_data['image_paths'][img_idx]
                    img = Image.open(img_path)
                    st.image(img, caption=os.path.basename(img_path), use_container_width=True)
                except Exception as e:
                    st.error(f"åŠ è½½å‡ºé”™: {e}")
